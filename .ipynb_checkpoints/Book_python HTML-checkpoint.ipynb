{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80373ce2-71df-413a-85fa-a942579c9016",
   "metadata": {},
   "source": [
    "___NETWORK PROGRAMMING:___ Internet sources of information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aac034-7f16-4b4b-86dc-5c2d8fc0b7a4",
   "metadata": {},
   "source": [
    "__Hypertext transfer protocol(HTTP):__ https://www.w3.org/Protocols/rfc2616/rfc2616.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5891e759-d468-473c-a130-8537fcc81438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Thu, 10 Jun 2021 18:35:32 GMT\n",
      "Server: Apache/2.4.18 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"a7-54f6609245537\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 167\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "# Manual way to retrive a text from a website -- for images look page 152 \n",
    "import socket\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #connecting as a web browser socket -- Send and recieve data\n",
    "mysock.connect(('data.pr4e.org', 80))  # website and port we will connect (http:80,https:443,ftp:21,email:25,etc) \n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode() # as web browser sending through http, we must use GET followed by a blankline \\r\\n\\r\\n\n",
    "mysock.send(cmd)\n",
    "while True:\n",
    "    data = mysock.recv(512) # a loop that recieves data in chunks of 512 characters from the socket\n",
    "    if len(data) < 1:       # the loop continue retriving until there are 0 characters left to read\n",
    "        break\n",
    "    print(data.decode(),end='') # switch the end default parameter from\\n to ''\n",
    "mysock.close()\n",
    "# THERE IS A EASY WAY USING URLIB !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0675334a-a2e1-48fb-bb2c-5f940d858386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "# retriving a text file from the web\n",
    "import urllib.request # lets do the same previous process but using urllib\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')# fhand has the info as though it were a file\n",
    "for line in fhand: # just make sure to decode !!\n",
    "    print(line.decode().strip()) # you need to decode beacuse the info comes in bytes ! (you'll see an object not a string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bc05d8d-6ea0-46dc-b9d4-d8aeaada61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriving a non-text file (binary) -- images,video,etc...\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "img = urllib.request.urlopen('http://data.pr4e.org/cover3.jpg').read() #saving the image data in a variable--you need to read it\n",
    "fhand = open('cover3.jpg', 'wb') #  wb means open with writing in a binary mode(b), as the file doesn't exist generates a new one in the path...\n",
    "fhand.write(img) # we are puting the binary data in the new document\n",
    "fhand.close()\n",
    "# but if the file is larger than your CPu memory the program will crash. So do it in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92b02c03-7c19-40cd-8370-9b9a4ac7f897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230210 characters copied.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "img = urllib.request.urlopen('http://data.pr4e.org/cover3.jpg') # saving the data in a variable\n",
    "fhand = open('cover3.jpg', 'wb') #opening a new file in writing and binary mood\n",
    "size = 0 \n",
    "while True:                   # loop for saving in chunks\n",
    "    info = img.read(100000)   # read the data (in img) by chunks of 100.000 characters\n",
    "    if len(info) < 1: break   # exhaust the data in img \n",
    "    size = size + len(info)   # count the amount of charchters -- optional\n",
    "    fhand.write(info)         # put each chunk in the new bianry file\n",
    "print(size, 'characters copied.')\n",
    "fhand.close()\n",
    "#obtain the same result as in the previous code but whithout a crashing risk !!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cb533b-2271-4f53-a205-7fd8756cdf87",
   "metadata": {
    "tags": []
   },
   "source": [
    "__Parsing and Scraping the web__ (HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16ee4b87-a75b-4196-b14e-c79a4d783fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter web site url https://docs.python.org/3/library/functions.html#open\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.python.org/3/library/functions.html\n",
      "https://www.python.org/\n",
      "https://www.python.org/dev/peps/pep-0475\n",
      "https://rhettinger.wordpress.com/2011/05/26/super-considered-super/\n",
      "https://www.python.org/dev/peps/pep-0302\n",
      "https://www.python.org/dev/peps/pep-0328\n",
      "https://github.com/python/cpython/blob/3.9/Doc/library/functions.rst\n",
      "https://www.python.org/\n",
      "https://www.python.org/psf/donations/\n",
      "https://docs.python.org/3/bugs.html\n",
      "https://www.sphinx-doc.org/\n"
     ]
    }
   ],
   "source": [
    "# retriving urls within a url -- using regular expressions\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "import ssl\n",
    "\n",
    "ctx = ssl.create_default_context() ## SSl certificate errors allows to access websites that extrictly enforce HTTPS.\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter web site url')\n",
    "html = urllib.request.urlopen(url,context = ctx).read() # save and read the dta in a variable\n",
    "links = re.findall(b'href=\"(http[s]?://.*?)\"', html) # b' beacus the data is coded. retrive all that contains “href=\"http://” \n",
    "                          # or “href=\"http://” ([s]?->followed by 0 or 1 s). Then, zero or more characters up to \" (.*\") in the\n",
    "for link in links:        # non-greedy  mood (?). stops at the first final\" t finds.\n",
    "    print(link.decode()) # remember to decode the data if want to see it as a string!!!\n",
    "# this only works when the HTML in the web is well formatted.Otherwise you'll skip important information.\n",
    "# Its better to use a parsing library: e.g., beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "894efb6d-a9f5-47ba-b0d3-922ee78d4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter -  http://www.dr-chuck.com/page1.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: <a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Second Page</a>\n",
      "URL: http://www.dr-chuck.com/page2.htm\n",
      "Body of tag: \n",
      "Second Page\n",
      "Attrs: {'href': 'http://www.dr-chuck.com/page2.htm'}\n"
     ]
    }
   ],
   "source": [
    "#Using a praser library -- BEAUTIFULSOUP\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "ctx = ssl.create_default_context()  # SSL Certificate errors\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ')  ## ask for a website url\n",
    "html = urllib.request.urlopen(url, context=ctx).read() #store the data in a variable\n",
    "soup = BeautifulSoup(html, 'html.parser')  # use bs4 to parse the data in the html mode and save it in a variable\n",
    "\n",
    "tags = soup('a')  # Retrieve all of the anchor tags of html (<a..>...</a>)\n",
    "for tag in tags:\n",
    "    print(\"Tag:\", tag) # the whole tag in the anhor \"a\"\n",
    "    print(\"URL:\", tag.get('href', None)) # get herf and if there isn't herf, retrives None.\n",
    "    print(\"Body of tag:\", tag.contents[0]) # contents retrives a list of one element. that's why [0]\n",
    "    print(\"Attrs:\", tag.attrs) # returns a dict info of the anchor.\n",
    "    \n",
    "# There is alot of usages that beautiful soup can do regarding to parse HTML. Read the documentation !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09762bc7-1295-41b6-9c1d-d9fc885e474e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter the url https://www.w3.org/Protocols/rfc2616/rfc2616.txt\n"
     ]
    }
   ],
   "source": [
    "#Exercise 1\n",
    "import socket\n",
    "import re\n",
    "web_ = input(\"enter the url\")\n",
    "protocol_list = re.findall(\"(http.*):\",web_)\n",
    "protocol = protocol_list[0].lower()\n",
    "port = 0\n",
    "if protocol == \"http\": port = 80\n",
    "else: port = 443\n",
    "body_list = re.findall(\"//(.+?)/\",web_)\n",
    "body = body_list[0]\n",
    "    \n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) \n",
    "mysock.connect((body, port)) \n",
    "cmd = 'GET '+str(web_)+' '+protocol.upper()+'/1.0\\r\\n\\r\\n'\n",
    "cmd = cmd.encode()\n",
    "mysock.send(cmd)\n",
    "while True:\n",
    "    data = mysock.recv(512) \n",
    "    if len(data) < 1:       \n",
    "        break\n",
    "    print(data.decode(),end='') #\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4f100a6-202f-4186-ae87-2d605d05527b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter the url https://en.wikipedia.org/wiki/Nero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of characters is: 3000 \n",
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Nero - Wikipedia</title>\n",
      "<script>document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":!1,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"8ef4c6ad-331f-454a-a03a-3076925f2453\",\"wgCSPNonce\":!1,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":!1,\"wgNamespaceNumber\":0,\"wgPageName\":\"Nero\",\"wgTitle\":\"Nero\",\"wgCurRevisionId\":1027854148,\"wgRevisionId\":1027854148,\"wgArticleId\":21632,\"wgIsArticle\":!0,\"wgIsRedirect\":!1,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Webarchive template other archives\",\"Webarchive template wayback links\",\"Webarchive template webcite links\",\"Webarchive template archiveis links\",\"Articles with short description\",\"Short description is different from Wikidata\",\"Wikipedia indefinitely semi-protected pages\",\"Use dmy dates from April 2020\",\n",
      "\"All articles with unsourced statements\",\"Articles with unsourced statements from June 2009\",\"Articles with unsourced statements from September 2017\",\"Articles needing additional references from June 2020\",\"All articles needing additional references\",\"Articles with French-language sources (fr)\",\"Wikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica with Wikisource reference\",\"Commons category link is on Wikidata\",\"Wikipedia articles with BIBSYS identifiers\",\"Wikipedia articles with BNC identifiers\",\"Wikipedia articles with BNE identifiers\",\"Wikipedia articles with BNF identifiers\",\"Wikipedia articles with CANTIC identifiers\",\"Wikipedia articles with FAST identifiers\",\"Wikipedia articles with GND identifiers\",\"Wikipedia articles with ISNI identifiers\",\"Wikipedia articles with LCCN identifiers\",\"Wikipedia articles with LNB identifiers\",\"Wikipedia articles with MusicBrainz identifiers\",\"Wikipedia articles with NDL identifiers\",\n",
      "\"Wikipedia articles with NKC identifiers\",\"Wikipedia articles with NLI identifiers\",\"Wikipedia articles with NLP identifiers\",\"Wikipedia articles with NTA identifiers\",\"Wikipedia articles with PLWABN identifiers\",\"Wikipedia articles with SELIBR identifiers\",\"Wikipedia articles with SNAC-ID identifiers\",\"Wikipedia articles with SUDOC identifiers\",\"Wikipedia articles with ULAN identifiers\",\"Wikipedia articles with VcBA identifiers\",\"Wikipedia articles with VIAF identifiers\",\"Wikipedia articles with WORLDCATID identifiers\",\"Wikipedia articles with multiple identifiers\",\"Julio-Claudian dynasty\",\"Nero\",\"37 births\",\"68 deaths\",\"1st-century Roman emperors\",\"Ancient LGBT people\",\"Ancient Roman adoptees\",\"Ancient Romans who committed suicide\",\"LGBT royalty\",\"Children of Claudius\",\"Claudii Nerones\",\"Domitii Ahenobarbi\",\"Forced suicides\",\"Italian rapists\",\"LGBT heads of state\",\"LGBT people from Italy\",\"Matricides\",\"Peop\n"
     ]
    }
   ],
   "source": [
    "#exercise 3\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "web_ = input(\"enter the url\")\n",
    "\n",
    "ctx = ssl.create_default_context()  # SSL Certificate errors\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "info = urllib.request.urlopen(web_, context=ctx)\n",
    "#html = BeautifulSoup(info, 'html.parser')\n",
    "counter = 0\n",
    "list_3000 = list()\n",
    "for line in info:\n",
    "    line = line.decode()\n",
    "    for each in line:\n",
    "        if len(each) > 0 and counter < 3000:\n",
    "            counter += 1\n",
    "            list_3000.append(each)\n",
    "        \n",
    "print ('The number of characters is: %d'%counter,\"\\n\"+\"\".join(list_3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b971469-250a-4a64-a923-71dee0f5e403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter -  https://en.wikipedia.org/wiki/Nero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paragraphs: 94\n"
     ]
    }
   ],
   "source": [
    "# exercise 4\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "ctx = ssl.create_default_context() \n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input('Enter - ') \n",
    "html = urllib.request.urlopen(url, context=ctx).read() \n",
    "soup = BeautifulSoup(html, 'html.parser') \n",
    "\n",
    "tags = soup.find_all('p')  \n",
    "counter = 0\n",
    "for tag in tags:\n",
    "    counter += 1 \n",
    "print(\"Number of paragraphs: %d\"%counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02f6c96-c05b-4782-b545-fee09e6069c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " dsf\n"
     ]
    }
   ],
   "source": [
    "http://data.pr4e.org/romeo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a830f59-fcb7-431e-9990-5d34a6d01413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80c585-ce86-474a-8628-647c051cb014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
